![image](https://github.com/user-attachments/assets/c502b76f-2aff-4338-8e4c-7860f9c582d2)

# Welcome to my project.

## Pipeline with Airflow, dbt, and Snowflake-


## Link del video explicando:


## Project Description
This project focuses on building a data pipeline leveraging Apache Airflow, dbt, and Snowflake, following the Quick Lab provided by Snowflake. The pipeline will perform extract, transform, and load (ETL) operations to manage data in a cloud-based data warehouse, adhering to collaboration best practices through GitHub.

---

## Objectives
- Automate data workflows using *Apache Airflow*.
- Transform raw data into meaningful insights with *dbt*.
- Store and manage data efficiently in *Snowflake*.
- Demonstrate teamwork through *GitHub commits* and *Pull Requests*.

---

## Requirements
To complete this project, you will need:
- *Docker*: To set up the environment and run containers.
- *Apache Airflow*: To orchestrate the workflow.
- *dbt (Data Build Tool)*: To perform transformations.
- *Snowflake*: As the data warehouse.
- *Python* and *SQL*: For scripting and queries.
- A *GitHub account*: For project collaboration and code management.

---


### Resources:
- Learn more about dbt [in the docs](https://docs.getdbt.com/docs/introduction)
- Check out [Discourse](https://discourse.getdbt.com/) for commonly asked questions and answers
- Join the [chat](https://community.getdbt.com/) on Slack for live discussions and support
- Find [dbt events](https://events.getdbt.com) near you
- Check out [the blog](https://blog.getdbt.com/) for the latest news on dbt's development and best practices
